{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "class Multi_fft_conv2d(nn.Module):\n",
    "    def  __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer1 = fft_conv2d()\n",
    "        self.pool1 = MaxPool2d_complex(40)\n",
    "        self.fc1 = nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = x.view((-1, 4 * 4 * 2))\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "\n",
    "class MaxPool2d_complex(nn.Module):\n",
    "    def  __init__(self, pool_size = 40):\n",
    "        nn.Module.__init__(self)\n",
    "        self.pool_size = pool_size\n",
    "    def forward(self, com_tensor):\n",
    "        tensor_r, tensor_j = com_tensor.split(1, -1)\n",
    "        tensor_r = tensor_r.squeeze(-1)\n",
    "        tensor_j = tensor_j.squeeze(-1)\n",
    "        pool = nn.MaxPool2d(self.pool_size)\n",
    "        tensor_pool = torch.stack([pool(tensor_r), pool(tensor_j)], dim=-1)\n",
    "        return tensor_pool\n",
    "\n",
    "\n",
    "class fft_conv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, tile_size = 40, kernel_size = 32, tiling_factor = 4):\n",
    "        super(fft_conv2d, self).__init__()\n",
    "        self.tile_size = tile_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.tiling_factor = tiling_factor\n",
    "\n",
    "        pad_one, pad_two = int(np.ceil((self.tile_size - self.kernel_size)/2)), int(np.floor((self.tile_size - self.kernel_size)//2))\n",
    "        # self.kernels_lists = [[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0 ,0 ,0]]\n",
    "        # for i in range(4):\n",
    "        #     for j in range(4):\n",
    "        #         names['kernels_' + str(i) + str(j)] = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))\n",
    "        kernels_lists = [[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0 ,0 ,0]]\n",
    "        \n",
    "        for i in range(self.tiling_factor):\n",
    "            for j in range(self.tiling_factor):\n",
    "                # self.names['kernels_' + str(i) + str(j)] = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))\n",
    "                exec('self.kernel_{}{} = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))'.format(i,j))\n",
    "                exec('kernels_lists[i][j] = self.kernel_{}{}'.format(i,j))\n",
    "        self.kernels_lists = kernels_lists\n",
    "        kernels_pad =[[F.pad(kernel, (0,0,pad_one,pad_two, pad_one,pad_two), 'constant', 0) for kernel in kernels]for kernels in                                    self.kernels_lists]\n",
    "        self.psf_pad = torch.cat([torch.cat(kernel_list, dim=0) for kernel_list in kernels_pad], dim=1)\n",
    "\n",
    "        # self.fc1 = nn.Sequential(\n",
    "        #     nn.Linear(32, 16),\n",
    "        #     # nn.ReLU()\n",
    "        # )\n",
    "        # self.fc2 = nn.Sequential(\n",
    "        #     nn.Linear(16, 10),\n",
    "        #     #nn.ReLU()\n",
    "        # )\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fftconv2d(x, self.psf_pad, otf=None, adjoint=False, phase=True)\n",
    "        # x = self.MaxPool2d_complex(x)\n",
    "        # x = x.view(-1, 4 * 8)\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.fc2(x)\n",
    "        # nn.MaxPool2d(40, 40)\n",
    "        # x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        # x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "    def torch_conj(self, a_tensor):\n",
    "\n",
    "        # 复共轭\n",
    "        otf_conj = torch.cat([a_tensor.index_select(-1, torch.tensor([0])), a_tensor.index_select(-1, torch.tensor([1])) * -1],dim = -1)\n",
    "        return otf_conj\n",
    "\n",
    "    def shift2d(self,a_tensor):\n",
    "\n",
    "        x_shift = (np.shape(a_tensor)[0]+1)//2\n",
    "        y_shift = (np.shape(a_tensor)[1]+1)//2\n",
    "        shift2d = torch.roll(a_tensor,shifts = (x_shift, y_shift), dims = (0,1))\n",
    "        return shift2d\n",
    "\n",
    "    def ishift2d(self, a_tensor):\n",
    "\n",
    "        x_shift = (np.shape(a_tensor)[0]+1)//2\n",
    "        y_shift = (np.shape(a_tensor)[1]+1)//2\n",
    "        ishift2d = torch.roll(a_tensor,shifts = (-x_shift, -y_shift), dims = (0,1))\n",
    "        return ishift2d\n",
    "\n",
    "    def psf2otf(self, psf, psf_size):\n",
    "\n",
    "        # psf = F.pad()\n",
    "        psf = self.ishift2d(psf)\n",
    "        otf = torch.fft(psf, signal_ndim = 2)\n",
    "        return otf\n",
    "\n",
    "    def fftconv2d(self, img, psf_pad, otf=None, adjoint=False, phase=True):\n",
    "        # real2complex\n",
    "        # if len(img.shape) == 4:\n",
    "        if img.shape[-1] != 2:\n",
    "            img_j = torch.zeros_like(img)\n",
    "            img = torch.stack([img, img_j], dim=-1)\n",
    "\n",
    "        pad_one, pad_two = int(np.ceil((self.tile_size * self.tiling_factor  - img.shape[-2])/2)), int(np.floor((self.tile_size * self.tiling_factor - img.shape[-3])//2))\n",
    "        img_pad = F.pad(img,(0, 0, pad_one, pad_two, pad_one, pad_two), \"constant\", 0)          \n",
    "        \n",
    "        img_pad_fft2d = torch.fft(img_pad, signal_ndim = 2)\n",
    "        \n",
    "        otf = self.psf2otf(psf_pad, psf_size = self.tile_size * self.tiling_factor)\n",
    "        otf = otf.unsqueeze(0)\n",
    "        \n",
    "        if adjoint:\n",
    "            result = torch.ifft(img_pad_fft2d * self.torch_conj(otf), signal_ndim = 2)\n",
    "        else:\n",
    "            result = torch.ifft(img_pad_fft2d * otf, signal_ndim = 2)\n",
    "\n",
    "        if phase:  \n",
    "            result = result\n",
    "        else:\n",
    "            result = index_select(-1, torch.tensor([0]))\n",
    "        \n",
    "\n",
    "        return result\n",
    "\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-08a47a43dbb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "source": [
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import model\n",
    "# import pdb\n",
    "\n",
    "\n",
    "\n",
    "# 超参数设置\n",
    "EPOCH = 100   #遍历数据集次数\n",
    "BATCH_SIZE = 128      #批处理尺寸(batch_size)\n",
    "LR = 0.001        #学习率\n",
    " \n",
    "\n",
    "\n",
    "DATASET = 'ucm'\n",
    "# pdb.set_trace()\n",
    "if DATASET == 'mnist':\n",
    "    # pdb.set_trace()\n",
    "    # 定义数据预处理方式\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # 定义训练数据集\n",
    "    trainset = tv.datasets.MNIST(root='/home/lichen/media/DataSets/',\n",
    "                                train=True,\n",
    "                                download= False,\n",
    "                                transform=transform)\n",
    "\n",
    "    # 定义训练批处理数据\n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True,\n",
    "                                            )\n",
    "    \n",
    "    # 定义测试数据集\n",
    "    testset = tv.datasets.MNIST(root='/home/lichen/media/DataSets/',\n",
    "                                train=False,\n",
    "                                download=False,\n",
    "                                transform=transform)\n",
    "    \n",
    "    # 定义测试批处理数据\n",
    "    testloader = torch.utils.data.DataLoader(testset,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False,\n",
    "                                            )\n",
    "    \n",
    "elif DATASET == 'ucm':\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            # transforms.Scale(256),\n",
    "            transforms.Scale(30),\n",
    "\t\t    # transforms.RandomSizedCrop(224),\n",
    "\t\t    transforms.RandomHorizontalFlip(),\n",
    "\t\t    transforms.ToTensor(),\n",
    "\t\t    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t\t]),\n",
    "        'val': transforms.Compose([\n",
    "\t\t    # transforms.Scale(256),\n",
    "            transforms.Scale(30),\n",
    "\t\t    # transforms.CenterCrop(30),\n",
    "\t\t    transforms.ToTensor(),\n",
    "\t\t    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t    ]),\n",
    "\t}\n",
    "    data_dir = '/Users/lichen/Downloads/DataSets/UCM/'\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "    trainloader = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True)for x in ['train', 'val']}\n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "# 定义损失函数loss function 和优化方式（采用SGD）\n",
    "# 定义是否使用GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "net = model.fft_conv2d().to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，通常用于多分类问题上\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # pdb.set_trace()\n",
    "    sum_loss = 0.0\n",
    "    # 数据读取\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每训练100个batch打印一次平均loss\n",
    "        sum_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %d] loss: %.03f'\n",
    "                    % (epoch + 1, i + 1, sum_loss / 100))\n",
    "            sum_loss = 0.0\n",
    "    # 每跑完一次epoch测试一下准确率\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            # 取得分最高的那个类\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print('第%d个epoch的识别准确率为：%d%%' % (epoch + 1, (100 * correct / total)))\n",
    "\n",
    ""
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e0cecaa0afae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "metadata": {},
   "execution_count": 1
  }
 ]
}