{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda8d522ecb3e5d4de9b140ec06314d2d05",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "EPOCH = 10   #遍历数据集次数\n",
    "BATCH_SIZE = 64      #批处理尺寸(batch_size)\n",
    "LR = 0.001        #学习率\n",
    " \n",
    "# 定义数据预处理方式\n",
    "transform = transforms.ToTensor()\n",
    " \n",
    "# 定义训练数据集\n",
    "trainset = tv.datasets.MNIST(root='/Users/lichen/Downloads/DataSets/',\n",
    "                             train=True,\n",
    "                             download= False,\n",
    "                             transform=transform)\n",
    "\n",
    "# 定义训练批处理数据\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          )\n",
    " \n",
    "# 定义测试数据集\n",
    "testset = tv.datasets.MNIST(root='/Users/lichen/Downloads/DataSets/',\n",
    "                            train=False,\n",
    "                            download=False,\n",
    "                            transform=transform)\n",
    " \n",
    "# 定义测试批处理数据\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数loss function 和优化方式（采用SGD）\n",
    "# 定义是否使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = fft_conv2d().to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，通常用于多分类问题上\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class fft_conv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, tile_size = 40, kernel_size = 32, tiling_factor = 4):\n",
    "        super(fft_conv2d, self).__init__()\n",
    "        self.tile_size = tile_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.tiling_factor = tiling_factor\n",
    "\n",
    "        pad_one, pad_two = int(np.ceil((self.tile_size - self.kernel_size)/2)), int(np.floor((self.tile_size - self.kernel_size)//2))\n",
    "        # self.kernels_lists = [[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0 ,0 ,0]]\n",
    "        # for i in range(4):\n",
    "        #     for j in range(4):\n",
    "        #         names['kernels_' + str(i) + str(j)] = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))\n",
    "        kernels_lists = [[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0 ,0 ,0]]\n",
    "        \n",
    "        for i in range(self.tiling_factor):\n",
    "            for j in range(self.tiling_factor):\n",
    "                # self.names['kernels_' + str(i) + str(j)] = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))\n",
    "                exec('self.kernel_{}{} = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))'.format(i,j))\n",
    "                exec('kernels_lists[i][j] = self.kernel_{}{}'.format(i,j))\n",
    "        self.kernels_lists = kernels_lists\n",
    "        kernels_pad =[[F.pad(kernel, (0,0,pad_one,pad_two, pad_one,pad_two), 'constant', 0) for kernel in kernels]for kernels in                                    self.kernels_lists]\n",
    "        self.psf_pad = torch.cat([torch.cat(kernel_list, dim=0) for kernel_list in kernels_pad], dim=1)\n",
    "        self.fc1 = nn.Linear(32, 10)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fftconv2d(x, self.psf_pad, otf=None, adjoint=False, phase=True)\n",
    "        x = self.MaxPool2d_complex(x)\n",
    "        x = x.view(-1, 4 * 8)\n",
    "        x = self.fc1(x)\n",
    "        # nn.MaxPool2d(40, 40)\n",
    "        # x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        # x = self.classifier(x)\n",
    "        return x\n",
    "    def MaxPool2d_complex(self, a_tensor):\n",
    "        # shape = (1,1,160,160,2)\n",
    "        a_r, a_j = a_tensor.split(1,-1)\n",
    "        a_r = a_r.squeeze(-1)\n",
    "        a_j = a_j.squeeze(-1)\n",
    "        pool = nn.MaxPool2d(40)\n",
    "        a_r_p = pool(a_r)\n",
    "        a_j_p= pool(a_j)\n",
    "        a_p = torch.stack([a_r_p, a_j_p], dim = -1)\n",
    "        \n",
    "        return a_p\n",
    "        \n",
    "    def torch_conj(self, a_tensor):\n",
    "\n",
    "        # 复共轭\n",
    "        otf_conj = torch.cat([a_tensor.index_select(-1, torch.tensor([0])), a_tensor.index_select(-1, torch.tensor([1])) * -1],dim = -1)\n",
    "        return otf_conj\n",
    "\n",
    "    def shift2d(self,a_tensor):\n",
    "\n",
    "        x_shift = (np.shape(a_tensor)[0]+1)//2\n",
    "        y_shift = (np.shape(a_tensor)[1]+1)//2\n",
    "        shift2d = torch.roll(a_tensor,shifts = (x_shift, y_shift), dims = (0,1))\n",
    "        return shift2d\n",
    "\n",
    "    def ishift2d(self, a_tensor):\n",
    "\n",
    "        x_shift = (np.shape(a_tensor)[0]+1)//2\n",
    "        y_shift = (np.shape(a_tensor)[1]+1)//2\n",
    "        ishift2d = torch.roll(a_tensor,shifts = (-x_shift, -y_shift), dims = (0,1))\n",
    "        return ishift2d\n",
    "\n",
    "    def psf2otf(self, psf, psf_size):\n",
    "\n",
    "        # psf = F.pad()\n",
    "        psf = self.ishift2d(psf)\n",
    "        otf = torch.fft(psf, signal_ndim = 2)\n",
    "        return otf\n",
    "\n",
    "    def fftconv2d(self, img, psf_pad, otf=None, adjoint=False, phase=True):\n",
    "        # real2complex\n",
    "        # if len(img.shape) == 4:\n",
    "        if img.shape[-1] != 2:\n",
    "            img_j = torch.zeros_like(img)\n",
    "            img = torch.stack([img, img_j], dim=-1)\n",
    "\n",
    "        pad_one, pad_two = int(np.ceil((self.tile_size * self.tiling_factor  - img.shape[-2])/2)), int(np.floor((self.tile_size * self.tiling_factor - img.shape[-3])//2))\n",
    "        img_pad = F.pad(img,(0, 0, pad_one, pad_two, pad_one, pad_two), \"constant\", 0)          \n",
    "        \n",
    "        img_pad_fft2d = torch.fft(img_pad, signal_ndim = 2)\n",
    "        \n",
    "        otf = self.psf2otf(psf_pad, psf_size = self.tile_size * self.tiling_factor)\n",
    "        otf = otf.unsqueeze(0)\n",
    "        \n",
    "        if adjoint:\n",
    "            result = torch.ifft(img_pad_fft2d * self.torch_conj(otf), signal_ndim = 2)\n",
    "        else:\n",
    "            result = torch.ifft(img_pad_fft2d * otf, signal_ndim = 2)\n",
    "\n",
    "        if phase:  \n",
    "            result = result\n",
    "        else:\n",
    "            result = index_select(-1, torch.tensor([0]))\n",
    "        return result\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1, 100] loss: 79.668\n[1, 200] loss: 70.823\n[1, 300] loss: 73.063\n[1, 400] loss: 70.470\n[1, 500] loss: 61.588\n[1, 600] loss: 67.565\n[1, 700] loss: 56.188\n[1, 800] loss: 54.585\n[1, 900] loss: 56.855\n第1个epoch的识别准确率为：10%\n[2, 100] loss: 47.346\n[2, 200] loss: 49.932\n[2, 300] loss: 45.507\n[2, 400] loss: 45.536\n[2, 500] loss: 49.106\n[2, 600] loss: 44.234\n[2, 700] loss: 52.730\n[2, 800] loss: 36.010\n[2, 900] loss: 38.502\n第2个epoch的识别准确率为：21%\n[3, 100] loss: 35.380\n[3, 200] loss: 33.931\n[3, 300] loss: 43.256\n[3, 400] loss: 38.258\n[3, 500] loss: 36.961\n[3, 600] loss: 33.906\n[3, 700] loss: 28.689\n[3, 800] loss: 31.661\n[3, 900] loss: 40.152\n第3个epoch的识别准确率为：20%\n[4, 100] loss: 36.800\n[4, 200] loss: 31.131\n[4, 300] loss: 36.500\n[4, 400] loss: 28.457\n[4, 500] loss: 33.064\n[4, 600] loss: 28.745\n[4, 700] loss: 27.273\n[4, 800] loss: 22.670\n[4, 900] loss: 26.961\n第4个epoch的识别准确率为：25%\n[5, 100] loss: 26.639\n[5, 200] loss: 26.100\n[5, 300] loss: 23.448\n[5, 400] loss: 22.558\n[5, 500] loss: 24.725\n[5, 600] loss: 28.234\n[5, 700] loss: 24.034\n[5, 800] loss: 26.028\n[5, 900] loss: 25.791\n第5个epoch的识别准确率为：20%\n[6, 100] loss: 26.574\n[6, 200] loss: 22.666\n[6, 300] loss: 24.608\n[6, 400] loss: 25.119\n[6, 500] loss: 31.806\n[6, 600] loss: 30.620\n[6, 700] loss: 21.851\n[6, 800] loss: 16.852\n[6, 900] loss: 16.669\n第6个epoch的识别准确率为：29%\n[7, 100] loss: 18.923\n[7, 200] loss: 20.158\n[7, 300] loss: 24.775\n[7, 400] loss: 20.353\n[7, 500] loss: 15.200\n[7, 600] loss: 16.293\n[7, 700] loss: 17.944\n[7, 800] loss: 13.633\n[7, 900] loss: 17.870\n第7个epoch的识别准确率为：31%\n[8, 100] loss: 11.784\n[8, 200] loss: 17.034\n[8, 300] loss: 12.062\n[8, 400] loss: 14.107\n[8, 500] loss: 7.447\n[8, 600] loss: 13.653\n[8, 700] loss: 15.350\n[8, 800] loss: 18.858\n[8, 900] loss: 12.126\n第8个epoch的识别准确率为：26%\n[9, 100] loss: 14.648\n[9, 200] loss: 18.174\n[9, 300] loss: 17.560\n[9, 400] loss: 10.102\n[9, 500] loss: 13.089\n[9, 600] loss: 14.874\n[9, 700] loss: 15.459\n[9, 800] loss: 10.011\n[9, 900] loss: 12.859\n第9个epoch的识别准确率为：34%\n[10, 100] loss: 8.881\n[10, 200] loss: 8.174\n[10, 300] loss: 15.133\n[10, 400] loss: 9.781\n[10, 500] loss: 9.937\n[10, 600] loss: 9.481\n[10, 700] loss: 11.012\n[10, 800] loss: 20.198\n[10, 900] loss: 10.908\n第10个epoch的识别准确率为：42%\n"
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "        sum_loss = 0.0\n",
    "        # 数据读取\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    " \n",
    "            # forward + backward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "            # 每训练100个batch打印一次平均loss\n",
    "            sum_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %d] loss: %.03f'\n",
    "                      % (epoch + 1, i + 1, sum_loss / 100))\n",
    "                sum_loss = 0.0\n",
    "        # 每跑完一次epoch测试一下准确率\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                # 取得分最高的那个类\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "            print('第%d个epoch的识别准确率为：%d%%' % (epoch + 1, (100 * correct / total)))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = fft_conv2d()\n",
    "input = torch.randn((12,1,160,160),requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 16])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(output)\n",
    "\n",
    "# img_pad_fft2d = torch.fft(ima_com, signal_ndim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-496ead025fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/lichen/Downloads/DataSets/UCM/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0;32m----> 3\u001b[0;31m \t\t\t\t\t\t\t\t\t\t\tdata_transforms[x]) for x in ['train', 'val']}\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-496ead025fc0>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/lichen/Downloads/DataSets/UCM/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0;32m----> 3\u001b[0;31m \t\t\t\t\t\t\t\t\t\t\tdata_transforms[x]) for x in ['train', 'val']}\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/lichen/Downloads/DataSets/UCM/'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "\t\t\t\t\t\t\t\t\t\t\tdata_transforms[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "otf_conj = torch.cat([a_tensor.index_select(-1, torch.tensor([0])), a_tensor.index_select(-1, torch.tensor([1])) * -1],dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_lists = layer.psf_pad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.randn(32,32,2)\n",
    "a_r, a_j = a.split(1,-1)\n",
    "# b = torch.transpose(a, -2,-1)\n",
    "# c = torch.transpose(b, -2,-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_r= a_r.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 32])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(a_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in layer.named_parameters():\n",
    "    print(name,':',parameter)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 40\n",
    "kernel_size = 32\n",
    "tiling_factor = 4\n",
    "pad_one, pad_two = int(np.ceil((tile_size - kernel_size)/2)), int(np.floor((tile_size - kernel_size)//2))\n",
    "kernels_lists = [[nn.Parameter(torch.rand(kernel_size, kernel_size, 2)) for i in range(tiling_factor)]for j in range(tiling_factor)]\n",
    "kernels_pad=[[F.pad(kernel, (0,0,pad_one,pad_two, pad_one,pad_two), 'constant', 0) for kernel in kernels]for kernels in kernels_lists]\n",
    "\n",
    "psf = torch.cat([torch.cat(kernel_list, dim=0) for kernel_list in kernels_pad], dim=1)\n",
    "\n",
    "psf_s = torch.fft(psf, signal_ndim = 2)\n",
    "\n",
    "x_shift = (np.shape(psf_s)[0]+1)//2\n",
    "y_shift = (np.shape(psf_s)[1]+1)//2\n",
    "\n",
    "psf_s_shift = torch.roll(psf_s, shifts = (x_shift, y_shift), dims = (0,1))\n",
    "\n",
    "# complex2real\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose (0, 1) dimentions as H and W\n",
    "tile_size = 40\n",
    "kernel_size = 32\n",
    "tiling_factor = 4\n",
    "\n",
    "def torch_conj(a_tensor):\n",
    "\n",
    "    # 复共轭\n",
    "    otf_conj = torch.cat([a_tensor.index_select(-1, torch.tensor([0])), a_tensor.index_select(-1, torch.tensor([1])) * -1],dim = -1)\n",
    "    return otf_conj\n",
    "\n",
    "def shift2d(a_tensor):\n",
    "\n",
    "    x_shift = (np.shape(a_tensor)[0]+1)//2\n",
    "    y_shift = (np.shape(a_tensor)[1]+1)//2\n",
    "    shift2d = torch.roll(a_tensor,shifts = (x_shift, y_shift), dims = (0,1))\n",
    "    return shift2d\n",
    "\n",
    "def ishift2d(a_tensor):\n",
    "\n",
    "    x_shift = (np.shape(a_tensor)[0]+1)//2\n",
    "    y_shift = (np.shape(a_tensor)[1]+1)//2\n",
    "    ishift2d = torch.roll(a_tensor,shifts = (-x_shift, -y_shift), dims = (0,1))\n",
    "    return ishift2d\n",
    "\n",
    "def ptf2otf(psf, psf_size):\n",
    "\n",
    "    # psf = F.pad()\n",
    "    psf = ishift2d(psf)\n",
    "    otf = torch.fft(tmp, signal_ndim = 2)\n",
    "    return otf\n",
    "\n",
    "def fft_conv2d(img_pad, psf_pad, otf=None, adjoint=False, phase=True):\n",
    "    # real2complex\n",
    "    img_pad_fft2d = torch.rfft(img_pad, signal_ndim = 2)\n",
    "    \n",
    "    otf = ptf2otf(psf_pad, psf_size = tile_size * tiling_factor)\n",
    "    \n",
    "    if adjoint:\n",
    "        result = torch.ifft(img_fft * torch_conj(otf), signal_ndim = 2)\n",
    "    else:\n",
    "        result = torch.ifft(img_fft * otf, signal_ndim = 2)\n",
    "\n",
    "    if phase:  \n",
    "        result = result\n",
    "    else:\n",
    "        result = index_select(-1, torch.tensor([0]))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.tensor([0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand(5,5,2)\n",
    "d = c.index_select(-1, torch.tensor([1])) * -1\n",
    "e = torch.cat([c.index_select(-1, torch.tensor([0])), c.index_select(-1, torch.tensor([1])) * -1],dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Parameter(torch.rand(5,5))\n",
    "\n",
    "a_fft = torch.rfft(a, signal_ndim = 2)\n",
    "\n",
    "a_shift = shift2d(a_fft)\n",
    "a_ishift = ishift2d(a_shift)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fft and shift\n",
    "a = nn.Parameter(torch.rand(5,5,2))\n",
    "print(a)\n",
    "a_s = torch.fft(a, signal_ndim = 2)\n",
    "print(a_s)\n",
    "x_shift = (np.shape(a_s)[0]+1)//2\n",
    "y_shift = (np.shape(a_s)[1]+1)//2\n",
    "a_shift = torch.roll(a_s, shifts = (x_shift, y_shift), dims = (0,1))\n",
    "print(a_s)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test image Fourier transform\n",
    "Ima = Image.open('3.bmp')\n",
    "Ima = Ima.convert('L')\n",
    "Ima_g = np.array(Ima)\n",
    "print(np.shape(Ima_g))\n",
    "Ima_g = torch.from_numpy(Ima_g)\n",
    "Ima_g = Ima_g.float()\n",
    "print(np.shape(Ima_g))\n",
    "Ima_r = torch.FloatTensor(Ima_g)\n",
    "Ima_j = torch.zeros(256,256)\n",
    "Ima_com = torch.cat([Ima_r.unsqueeze(-1), Ima_j.unsqueeze(-1)], dim=2)\n",
    "Ima_s = torch.fft(Ima_com, signal_ndim = 2)\n",
    "print(np.shape(Ima_s))\n",
    "x_shift = (np.shape(Ima_s)[0]+1)//2\n",
    "y_shift = (np.shape(Ima_s)[1]+1)//2\n",
    "Ima_shift = torch.roll(Ima_s, shifts = (x_shift, y_shift), dims = (0,1))\n",
    "print(np.shape(Ima_shift))\n",
    "Ima_shift_np = np.array(Ima_shift)\n",
    "Ima_shift_np = np.sqrt(np.square(Ima_shift_np[:,:,0])+np.square(Ima_shift_np[:,:,1]))\n",
    "# Ima_png = np.abs(Ima_shift)\n",
    "# Ima_png = Ima_png.numpy()\n",
    "img = Image.fromarray(Ima_shift_np)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = [[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0 ,0 ,0]]\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        kernel[i][j] = nn.Parameter(torch.rand(40, 40, 2))       "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fft_conv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, tile_size, kernel_size, tiling_factor):\n",
    "        names = self.__dict__\n",
    "        super(fft_conv2d, self).__init__()\n",
    "        self.names = names\n",
    "        self.tile_size = tile_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.tiling_factor = tiling_factor\n",
    "        # self.w = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))\n",
    "        pad_one, pad_two = int(np.ceil((self.tile_size - self.kernel_size)/2)), int(np.floor((self.tile_size - self.kernel_size)//2))\n",
    "\n",
    "        self.kernels_lists = [[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0, 0 ,0],[0, 0 ,0 ,0]]\n",
    "        \n",
    "        for i in range(self.tiling_factor):\n",
    "            for j in range(self.tiling_factor):\n",
    "                # self.names['kernels_' + str(i) + str(j)] = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))\n",
    "                exec('self.kernel_{}{} = nn.Parameter(torch.rand(self.kernel_size, self.kernel_size, 2))'.format(i,j))\n",
    "                exec('self.kernels_lists[i][j] = self.kernel_{}{}'.format(i,j))\n",
    "        kernels_pad =[[F.pad(kernel, (0,0,pad_one,pad_two, pad_one,pad_two), 'constant', 0) for kernel in kernels]for kernels in                                    self.kernels_lists]\n",
    "        self.psf_pad = torch.cat([torch.cat(kernel_list, dim=0) for kernel_list in kernels_pad], dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = fft_conv2d(40 ,32 ,4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fff.psf_pad"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in fff.named_parameters():\n",
    "    print(name,':',parameter)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "\n",
    "    for j in range(2):\n",
    "        b = torch.randn(5,5)\n",
    "        if j > 0:\n",
    "            b = torch.cat([b, b], dim = )\n",
    "             \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(5,5)\n",
    "c= b.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "np.shape(b)\n",
    "np.shape(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([c,b],dim =-1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Linear"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    pass"
   ]
  }
 ]
}